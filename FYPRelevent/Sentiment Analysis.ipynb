{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sentiment Analysis.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNKm42u4JqLpxEBiy0xP68P"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"d2PFcIAigMt7","colab_type":"code","outputId":"5dad8f95-a366-415d-955a-28daeb1440d3","executionInfo":{"status":"ok","timestamp":1583392471591,"user_tz":-300,"elapsed":8284,"user":{"displayName":"rana hamza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiIrfLEyuBvFQwn9bTpzgUo-TQU553I7gZArWR=s64","userId":"09989649813058534111"}},"colab":{"base_uri":"https://localhost:8080/","height":163}},"source":[""],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting preprocessor\n","  Downloading https://files.pythonhosted.org/packages/96/ad/d9f4ffb9bb97d1cb5bcb876b7932571d4dbaa3eff1701ad45d367f0ea27b/preprocessor-1.1.3.tar.gz\n","Building wheels for collected packages: preprocessor\n","  Building wheel for preprocessor (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for preprocessor: filename=preprocessor-1.1.3-cp36-none-any.whl size=4478 sha256=72bd1f7daa4a6557f8d98e9de66db9a316bdace0ad540b888fe9b4506822ac6a\n","  Stored in directory: /root/.cache/pip/wheels/98/c1/a2/21fbcfd80d76576bbf148991a66f00730f541f265c7600000f\n","Successfully built preprocessor\n","Installing collected packages: preprocessor\n","Successfully installed preprocessor-1.1.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2CaOGF84dmYb","colab_type":"code","outputId":"41f539c5-e2c6-4659-b4df-13636d7bdf0b","executionInfo":{"status":"error","timestamp":1591809199186,"user_tz":-300,"elapsed":2905,"user":{"displayName":"rana hamza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiIrfLEyuBvFQwn9bTpzgUo-TQU553I7gZArWR=s64","userId":"09989649813058534111"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"source":["import os\n","import tweepy\n","from tweepy import Stream\n","from tweepy import OAuthHandler\n","from tweepy.streaming import StreamListener\n","import json\n","import pandas as pd\n","import csv\n","import re #regular expression\n","from textblob import TextBlob\n","import string\n","import preprocessor as p\n"],"execution_count":1,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-bce22468b566>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'preprocessor'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"t-QBU6KjK9fF","colab_type":"code","colab":{}},"source":["# twitter API keys\n","consumer_key = \"PxmYFn0nSFr2fDOQp5zcVpaJI\"\n","consumer_secret = \"lUREJqEBfpylD2FdmDHXDUCRt1O9guft4NwDzPVQzfmiHkcbWm\"\n","access_key = \"2322684824-IdzgVAAFm35pK7NS7usMkumZ1esyEDexfsSKUwh\"\n","access_secret = \"4MFKvEk2ULASS27sUvtgGUqwDpLiDwBEd7LikzKgp9TGH\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QcnP2nkDdpX7","colab_type":"code","colab":{}},"source":["\n","auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n","auth.set_access_token(access_key, access_secret)\n","api = tweepy.API(auth)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L74XJITPeHAy","colab_type":"code","colab":{}},"source":["psl_tweets = \"data/psl_data_extraction/psl_data.csv\"\n","auratmarch_tweets = \"data/auratmarch_data_extraction/auratmarch_data.csv\"\n","worldcup_tweets = \"data/worldcup_data_extraction/worldcup_tweets_data.csv\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iCD3MIKaffhx","colab_type":"code","colab":{}},"source":["COLS = ['id', 'created_at', 'source', 'original_text','clean_text', 'sentiment','polarity','subjectivity', 'lang',\n","'favorite_count', 'retweet_count', 'original_author',   'possibly_sensitive', 'hashtags',\n","'user_mentions', 'place', 'place_coord_boundaries']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yx_0ubo3i8tq","colab_type":"code","colab":{}},"source":["#HappyEmoticons\n","emoticons_happy = set([\n","    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n","    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n","    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n","    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n","    '<3'\n","    ])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z2diSe6QlIzG","colab_type":"code","colab":{}},"source":["# Sad Emoticons\n","emoticons_sad = set([\n","    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n","    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n","    ':c', ':{', '>:\\\\', ';('\n","    ])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1q77As4hlO8O","colab_type":"code","colab":{}},"source":["#Emoji patterns\n","emoji_pattern = re.compile(\"[\"\n","         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","         u\"\\U00002702-\\U000027B0\"\n","         u\"\\U000024C2-\\U0001F251\"\n","         \"]+\", flags=re.UNICODE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hAiWtedglPi8","colab_type":"code","colab":{}},"source":["#combine sad and happy emoticons\n","emoticons = emoticons_happy.union(emoticons_sad)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"36zLH3DalSmg","colab_type":"code","colab":{}},"source":["def clean_tweets(tweet):\n"," \n","    stop_words = set(stopwords.words('english'))\n","    word_tokens = word_tokenize(tweet)\n","#after tweepy preprocessing the colon symbol left remain after      #removing mentions\n","    tweet = re.sub(r':', '', tweet)\n","    tweet = re.sub(r'‚Ä¶', '', tweet)\n","#replace consecutive non-ASCII characters with a space\n","    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n","#remove emojis from tweet\n","    tweet = emoji_pattern.sub(r'', tweet)\n","#filter using NLTK library append it to a string\n","    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n","    filtered_tweet = []\n","#looping through conditions\n","    for w in word_tokens:\n","#check tokens against stop words , emoticons and punctuations\n","        if w not in stop_words and w not in emoticons and w not in string.punctuation:\n","            filtered_tweet.append(w)\n","    return ' '.join(filtered_tweet)\n","    #print(word_tokens)\n","    #print(filtered_sentence)return tweet"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5A5x7IlLlXqB","colab_type":"code","colab":{}},"source":["#method write_tweets()\n","def write_tweets(keyword, file):\n","    # If the file exists, then read the existing data from the CSV file.\n","    if os.path.exists(file):\n","        df = pd.read_csv(file, header=0)\n","    else:\n","        df = pd.DataFrame(columns=COLS)\n","    #page attribute in tweepy.cursor and iteration\n","    for page in tweepy.Cursor(api.search, q=keyword,\n","                              count=200, include_rts=False, since=start_date).pages(50):\n","        for status in page:\n","            new_entry = []\n","            status = status._json\n","\n","            ## check whether the tweet is in english or skip to the next tweet\n","            if status['lang'] != 'en':\n","                continue\n","\n","            #when run the code, below code replaces the retweet amount and\n","            #no of favorires that are changed since last download.\n","            if status['created_at'] in df['created_at'].values:\n","                i = df.loc[df['created_at'] == status['created_at']].index[0]\n","                if status['favorite_count'] != df.at[i, 'favorite_count'] or \\\n","                   status['retweet_count'] != df.at[i, 'retweet_count']:\n","                    df.at[i, 'favorite_count'] = status['favorite_count']\n","                    df.at[i, 'retweet_count'] = status['retweet_count']\n","                continue\n","\n","\n","           #tweepy preprocessing called for basic preprocessing\n","            clean_text = p.clean(status['text'])\n","\n","            #call clean_tweet method for extra preprocessing\n","            filtered_tweet=clean_tweets(clean_text)\n","\n","            #pass textBlob method for sentiment calculations\n","            blob = TextBlob(filtered_tweet)\n","            Sentiment = blob.sentiment\n","\n","            #seperate polarity and subjectivity in to two variables\n","            polarity = Sentiment.polarity\n","            subjectivity = Sentiment.subjectivity\n","\n","            #new entry append\n","            new_entry += [status['id'], status['created_at'],\n","                          status['source'], status['text'],filtered_tweet, Sentiment,polarity,subjectivity, status['lang'],\n","                          status['favorite_count'], status['retweet_count']]\n","\n","            #to append original author of the tweet\n","            new_entry.append(status['user']['screen_name'])\n","\n","            try:\n","                is_sensitive = status['possibly_sensitive']\n","            except KeyError:\n","                is_sensitive = None\n","            new_entry.append(is_sensitive)\n","\n","            # hashtagas and mentiones are saved using comma separted\n","            hashtags = \", \".join([hashtag_item['text'] for hashtag_item in status['entities']['hashtags']])\n","            new_entry.append(hashtags)\n","            mentions = \", \".join([mention['screen_name'] for mention in status['entities']['user_mentions']])\n","            new_entry.append(mentions)\n","\n","            #get location of the tweet if possible\n","            try:\n","                location = status['user']['location']\n","            except TypeError:\n","                location = ''\n","            new_entry.append(location)\n","\n","            try:\n","                coordinates = [coord for loc in status['place']['bounding_box']['coordinates'] for coord in loc]\n","            except TypeError:\n","                coordinates = None\n","            new_entry.append(coordinates)\n","\n","            single_tweet_df = pd.DataFrame([new_entry], columns=COLS)\n","            df = df.append(single_tweet_df, ignore_index=True)\n","            csvFile = open(file, 'a' ,encoding='utf-8')\n","    df.to_csv(csvFile, mode='a', columns=COLS, index=False, encoding=\"utf-8\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pw1FRN5Flene","colab_type":"code","colab":{}},"source":["#declare keywords as a query for three categories\n","psl_keywords = '#telemedicine OR #telehealth OR #digitalhealth OR #ehealth OR #digitalpatient OR #digitaltransformation'\n","auratmarch_keywords = '#Epilepsy OR #epilepsyawareness OR #epilepsyaction OR #epilepsyalerts OR #epilepsybed OR #epilepsycongres OR #epilepsysurgery OR #epilepsysurgery OR #Epilepsytreatment OR #seizures OR #seizurefree'\n","worldcup_keywords = '#HeartDisease OR #stroke OR #Stroking OR #strokepatient OR #StrokeSurvivor OR #hearthealth OR #Stroke OR #HeartFailure'\n","\n","#call main method passing keywords and file path\n","write_tweets(psl_keywords,  psl_tweets)\n","write_tweets(auratmarch_keywords, auratmarch_tweets)\n","write_tweets(worldcup_keywords, worldcup_tweets)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6HOzAr7XnxnI","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}